{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import _pickle as pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import operator\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Dropout, Activation, Input, Flatten, Concatenate, GlobalAveragePooling1D, LSTM, GRU, Bidirectional, dot, multiply, Lambda, TimeDistributed, Masking\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2,l1,l1_l2\n",
    "from keras.callbacks import Callback,EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, precision_recall_curve, classification_report,accuracy_score, auc, roc_curve, roc_auc_score, average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    f = open(data_name,'rb')\n",
    "    data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('data_20200212_2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_para = []\n",
    "labels = []\n",
    "for d in data:\n",
    "    text_para.append(d[0])\n",
    "    labels.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doc(x):\n",
    "    return ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_doc = []\n",
    "for d in text_para:\n",
    "    text_doc.append(generate_doc(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = load_data('label_20200206_2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('labels',len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para_text = load_data('para_20200206_2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('para',len(para_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_text = load_data('doc_20200206_2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('doc',len(doc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARA_LENGTH = 250\n",
    "MAX_PARAS = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "#l1_reg = l1(1e-5)\n",
    "l12_reg = l1_l2(l1 = 1e-6,l2 = 1e-6)\n",
    "BATCH_SIZE = 24\n",
    "#metrics_auc = {}\n",
    "metrics_prcs = {}\n",
    "num_folder = 5\n",
    "#date = '20200208'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 201564 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_50000 = { k:v for (k,v) in word_index.items() if v<=50000 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jujun/fraudprediction_10k/HAN/input_data/word_index_50000_20200212','wb') as fp:\n",
    "    pickle.dump(word_index_50000,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/jujun/fraudprediction_10k/HAN/input_data/word_index_50000_20200207','rb') as fp:\n",
    "#     word_index_5000 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_index_50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(value):\n",
    "    return {k:v for k,v in word_index_50000.items() if v == value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((len(labels), MAX_PARAS, MAX_PARA_LENGTH), dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56288, 200, 250)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(text_para):\n",
    "    #structure of a doc: [p1,p2,p3,p4]\n",
    "    for j, para in enumerate(doc):\n",
    "        \n",
    "        if j < MAX_PARAS:\n",
    "            wordTokens = text_to_word_sequence(para)\n",
    "            \n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_PARA_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load('data_20200206.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_w2v = [x.split(' ') for x in text_doc]\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get my W2V model\n",
    "#Parameter: window=5,size=EMBEDDING_DIM, sg=1, workers=os.cpu_count(),min_count=5\n",
    "w2v_model = Word2Vec(doc_w2v, window=5, workers=os.cpu_count(), min_count=5, size=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.random.random(((MAX_NB_WORDS+ 1) , EMBEDDING_DIM))\n",
    "ignored=[]\n",
    "cnt=0\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv.vocab and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "        embedding_vector = w2v_model[word]\n",
    "        cnt+=1\n",
    "        if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    else: ignored.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120152"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49334"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embedding_matrix_20200212', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('w2v_model_202002012.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.load('embedding_matrix_20200206.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measure(pred_yp, y):\n",
    "    '''\n",
    "    Given lists of predicted y probability and x, y, return a dataframe of AR, AUC, Brier, Decile Table\n",
    "    '''\n",
    "    \n",
    "    tenc_dat = pd.DataFrame({'y_true':y,'probability':pred_yp.flatten()})\n",
    "    tenc_dat.sort_values('probability',axis = 0,ascending=False, inplace = True)\n",
    "    tenc_dat.index = range(0,len(tenc_dat))\n",
    "    y = tenc_dat['y_true']\n",
    "    point = float(len(tenc_dat))/10\n",
    "    point = int(round(point))\n",
    "    tenc = []\n",
    "    for i in range(0,10):\n",
    "        tenc.append(y[(i*point):((i+1)*point)])\n",
    "    tenc[9]=tenc[9].append(y[10*point:])\n",
    "    total = sum(y)\n",
    "    num_of_bkr = []\n",
    "    for j in range(0,10):\n",
    "        num_of_bkr.append(sum(tenc[j]))\n",
    "    tencile_bkr = np.array(num_of_bkr)\n",
    "    rate = tencile_bkr.astype(float)/total\n",
    "\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUCEvaluation(Callback):\n",
    "    \"\"\" Show AUC after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logs['auc'] = score\n",
    "            tencile=performance_measure(y_pred, self.y_val)\n",
    "            logs['tencile'] = tencile\n",
    "            print(\" epoch:{:d} AUC: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionEvaluation(Callback):\n",
    "    \"\"\" Show average Precision after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = average_precision_score(self.y_val, y_pred)\n",
    "            logs['Avg_Prec'] = score\n",
    "            #tencile=performance_measure(y_pred, self.y_val)\n",
    "            #logs['tencile'] = tencile\n",
    "            #print('prec_socre',score)\n",
    "            print(\" epoch:{:d} Avg_Prec: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUCEvaluation(Callback):\n",
    "    \"\"\" Show AUC after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logs['auc'] = score\n",
    "            tencile=performance_measure(y_pred, self.y_val)\n",
    "            logs['tencile'] = tencile\n",
    "            print(\" epoch:{:d} AUC: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionEvaluation(Callback):\n",
    "    \"\"\" Show average Precision after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = average_precision_score(self.y_val, y_pred)\n",
    "            logs['Avg_Prec'] = score\n",
    "            #tencile=performance_measure(y_pred, self.y_val)\n",
    "            #logs['tencile'] = tencile\n",
    "            #print('prec_socre',score)\n",
    "            print(\" epoch:{:d} Avg_Prec: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    \n",
    "    def __init__(self, regularizer=None,context_dim=100, name=\"attention\",**kwargs):\n",
    "        self.regularizer = regularizer\n",
    "        self.context_dim=context_dim\n",
    "        self.supports_masking = True\n",
    "        self.name=name\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3        \n",
    "        self.W = self.add_weight(name='W', shape=(input_shape[-1], self.context_dim), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.b = self.add_weight(name='b', shape=(self.context_dim,), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.u = self.add_weight(name='u', shape=(self.context_dim,), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)        \n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        eij = K.squeeze(K.dot(eij, K.expand_dims(self.u, axis=1)), axis=-1)\n",
    "        #eij = K.dot(K.tanh(K.dot(x, self.W) + self.b), self.u)\n",
    "        ai = K.exp(eij)\n",
    "        #print(\"ai size\", K.int_shape(ai))\n",
    "        #print(\"mask size\", K.int_shape(mask))\n",
    "        \n",
    "        if mask is not None:\n",
    "            ai*=K.cast(mask, K.floatx())\n",
    "            \n",
    "        ai /=K.cast(K.sum(ai, axis=1, keepdims=True)+K.epsilon(), K.floatx())\n",
    "        #weights=K.expand_dims(weights, axis=2) # (None, 1000,1)\n",
    "        #alphas = ai / K.sum(ai, axis=1).dimshuffle(0, 'x')\n",
    "        \n",
    "        #weighted_input = x * K.expand_dims(ai)\n",
    "        #return K.sum(weighted_input, axis=1)\n",
    "        return ai\n",
    "        \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(AttLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(Layer):\n",
    "    def __init__(self, name=\"weighted_sum\",  **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.name=name\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #assert isinstance(input_shape, list)\n",
    "        #print(\"input shape\", input_shape)\n",
    "        super(WeightedSum, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        \n",
    "        x = input_tensor[0]\n",
    "        #print(\"input\",K.int_shape(x))\n",
    "        a = input_tensor[1]\n",
    "        print(\"weights\", K.int_shape(a))\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = K.sum(x * a, axis=1)\n",
    "        #print(\"weighted sum\",K.int_shape(weighted_input))\n",
    "        \n",
    "        return weighted_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        a, b = input_shape\n",
    "        \n",
    "        return (a[0], a[-1])\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicModel(embedding_matrix,MAX_NB_WORDS,MAX_PARA_LENGTH,):\n",
    "    embedding_layer = Embedding((MAX_NB_WORDS+ 1),\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            mask_zero=True,\n",
    "                            input_length=MAX_PARA_LENGTH,\n",
    "                            trainable=True)    \n",
    "    \n",
    "    \n",
    "    para_input = Input(shape=(MAX_PARA_LENGTH, ), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(para_input)\n",
    "    norm_sequence = BatchNormalization()(embedded_sequences)\n",
    "    drop_out = Dropout(0.1)(norm_sequence)     \n",
    "    l_att = AttLayer(regularizer = l12_reg)(drop_out)\n",
    "    weighted_sum = WeightedSum()([norm_sequence,l_att])\n",
    "    paraEncoder =Model(para_input,weighted_sum)\n",
    "    paraEncoder.summary()\n",
    "    \n",
    "    doc_input = Input(shape=(MAX_PARAS, MAX_PARA_LENGTH), dtype='int32')\n",
    "    doc_encoder = TimeDistributed(paraEncoder)(doc_input)\n",
    "    mask_doc = Masking(mask_value=0.0)(doc_encoder)\n",
    "    norm_doc = BatchNormalization()(mask_doc)\n",
    "    l_lstm_para = GRU(100, return_sequences=True,implementation=2,kernel_regularizer=l12_reg)(norm_doc)\n",
    "    norm_doc_1 = BatchNormalization()(l_lstm_para)\n",
    "    l_att_para = AttLayer(regularizer=l12_reg)(norm_doc_1)\n",
    "    weighted_sum_doc = WeightedSum()([norm_doc_1, l_att_para])\n",
    "    batch_norm_doc = BatchNormalization()(weighted_sum_doc)\n",
    "    drop_out_doc = Dropout(0.1)(batch_norm_doc)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid',kernel_regularizer=l12_reg)(drop_out_doc) \n",
    "\n",
    "    model = Model(doc_input, preds)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(x_train,y_train, Best_Model_Filepath,model,x_val, y_val):\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    auc_eval = AUCEvaluation(validation_data=(x_val, y_val), interval=1)\n",
    "    precision_eval = PrecisionEvaluation(validation_data=(x_val, y_val), interval=1)\n",
    "    \n",
    "    earlyStopping = EarlyStopping(monitor='Avg_Prec',patience = 5, verbose =2, mode ='max')\n",
    "    checkpoint = ModelCheckpoint(Best_Model_Filepath, monitor='Avg_Prec', verbose=2, save_best_only=True, mode ='max')\n",
    "    print(\"training start...\")\n",
    "    training = model.fit(x_train,y_train,epochs=50,batch_size=BATCH_SIZE, \\\n",
    "                         callbacks=[auc_eval, precision_eval, earlyStopping, checkpoint],class_weight = class_weights,validation_data=[x_val,y_val],verbose=2)\n",
    "    #auc_eval,\n",
    "    \n",
    "    print('training end...')\n",
    "    \n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data,labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelName = '/home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.Series(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45030,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test= pd.Series(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights (None, 250)\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 250, 100)     5000100     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 250, 100)     400         embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 250, 100)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_5 (AttLayer)          (None, 250)          10200       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "weighted_sum_5 (WeightedSum)    (None, 100)          0           batch_normalization_9[0][0]      \n",
      "                                                                 att_layer_5[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 5,010,700\n",
      "Trainable params: 5,010,500\n",
      "Non-trainable params: 200\n",
      "__________________________________________________________________________________________________\n",
      "weights (None, 250)\n",
      "weights (None, 200)\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 200, 250)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 200, 100)     5010700     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_3 (Masking)             (None, 200, 100)     0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 200, 100)     400         masking_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, 200, 100)     60300       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 200, 100)     400         gru_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_6 (AttLayer)          (None, 200)          10200       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "weighted_sum_6 (WeightedSum)    (None, 100)          0           batch_normalization_11[0][0]     \n",
      "                                                                 att_layer_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 100)          400         weighted_sum_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 100)          0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            101         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,082,501\n",
      "Trainable params: 5,081,701\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n",
      "training start...\n",
      "Train on 45030 samples, validate on 11258 samples\n",
      "Epoch 1/50\n",
      " - 1215s - loss: 0.0932 - acc: 0.9805 - val_loss: 0.0587 - val_acc: 0.9913\n",
      " epoch:0 AUC: 0.5321\n",
      " epoch:0 Avg_Prec: 0.0102\n",
      "\n",
      "Epoch 00001: Avg_Prec improved from -inf to 0.01019, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 2/50\n",
      " - 1145s - loss: 0.0562 - acc: 0.9907 - val_loss: 0.0521 - val_acc: 0.9913\n",
      " epoch:1 AUC: 0.6589\n",
      " epoch:1 Avg_Prec: 0.0199\n",
      "\n",
      "Epoch 00002: Avg_Prec improved from 0.01019 to 0.01988, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 3/50\n",
      " - 1142s - loss: 0.0528 - acc: 0.9908 - val_loss: 0.0493 - val_acc: 0.9913\n",
      " epoch:2 AUC: 0.7509\n",
      " epoch:2 Avg_Prec: 0.0318\n",
      "\n",
      "Epoch 00003: Avg_Prec improved from 0.01988 to 0.03180, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 4/50\n",
      " - 1135s - loss: 0.0459 - acc: 0.9912 - val_loss: 0.0495 - val_acc: 0.9910\n",
      " epoch:3 AUC: 0.7723\n",
      " epoch:3 Avg_Prec: 0.0770\n",
      "\n",
      "Epoch 00004: Avg_Prec improved from 0.03180 to 0.07697, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 5/50\n",
      " - 1140s - loss: 0.0373 - acc: 0.9915 - val_loss: 0.0468 - val_acc: 0.9915\n",
      " epoch:4 AUC: 0.8372\n",
      " epoch:4 Avg_Prec: 0.2155\n",
      "\n",
      "Epoch 00005: Avg_Prec improved from 0.07697 to 0.21554, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 6/50\n",
      " - 1164s - loss: 0.0277 - acc: 0.9936 - val_loss: 0.0442 - val_acc: 0.9911\n",
      " epoch:5 AUC: 0.8483\n",
      " epoch:5 Avg_Prec: 0.2559\n",
      "\n",
      "Epoch 00006: Avg_Prec improved from 0.21554 to 0.25595, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 7/50\n",
      " - 1192s - loss: 0.0208 - acc: 0.9955 - val_loss: 0.0459 - val_acc: 0.9905\n",
      " epoch:6 AUC: 0.8582\n",
      " epoch:6 Avg_Prec: 0.3285\n",
      "\n",
      "Epoch 00007: Avg_Prec improved from 0.25595 to 0.32854, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 8/50\n",
      " - 1137s - loss: 0.0167 - acc: 0.9967 - val_loss: 0.0551 - val_acc: 0.9893\n",
      " epoch:7 AUC: 0.8254\n",
      " epoch:7 Avg_Prec: 0.2852\n",
      "\n",
      "Epoch 00008: Avg_Prec did not improve from 0.32854\n",
      "Epoch 9/50\n",
      " - 1153s - loss: 0.0138 - acc: 0.9973 - val_loss: 0.0529 - val_acc: 0.9929\n",
      " epoch:8 AUC: 0.8431\n",
      " epoch:8 Avg_Prec: 0.3590\n",
      "\n",
      "Epoch 00009: Avg_Prec improved from 0.32854 to 0.35899, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 10/50\n",
      " - 1151s - loss: 0.0135 - acc: 0.9975 - val_loss: 0.0531 - val_acc: 0.9922\n",
      " epoch:9 AUC: 0.8606\n",
      " epoch:9 Avg_Prec: 0.3392\n",
      "\n",
      "Epoch 00010: Avg_Prec did not improve from 0.35899\n",
      "Epoch 11/50\n",
      " - 1142s - loss: 0.0131 - acc: 0.9979 - val_loss: 0.0502 - val_acc: 0.9922\n",
      " epoch:10 AUC: 0.8603\n",
      " epoch:10 Avg_Prec: 0.4039\n",
      "\n",
      "Epoch 00011: Avg_Prec improved from 0.35899 to 0.40390, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 12/50\n",
      " - 1135s - loss: 0.0115 - acc: 0.9984 - val_loss: 0.0511 - val_acc: 0.9900\n",
      " epoch:11 AUC: 0.8513\n",
      " epoch:11 Avg_Prec: 0.4160\n",
      "\n",
      "Epoch 00012: Avg_Prec improved from 0.40390 to 0.41598, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 13/50\n",
      " - 1133s - loss: 0.0110 - acc: 0.9984 - val_loss: 0.0519 - val_acc: 0.9927\n",
      " epoch:12 AUC: 0.8569\n",
      " epoch:12 Avg_Prec: 0.4235\n",
      "\n",
      "Epoch 00013: Avg_Prec improved from 0.41598 to 0.42352, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 14/50\n",
      " - 1134s - loss: 0.0122 - acc: 0.9984 - val_loss: 0.0545 - val_acc: 0.9922\n",
      " epoch:13 AUC: 0.8472\n",
      " epoch:13 Avg_Prec: 0.4162\n",
      "\n",
      "Epoch 00014: Avg_Prec did not improve from 0.42352\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1148s - loss: 0.0104 - acc: 0.9988 - val_loss: 0.0545 - val_acc: 0.9923\n",
      " epoch:14 AUC: 0.8632\n",
      " epoch:14 Avg_Prec: 0.3753\n",
      "\n",
      "Epoch 00015: Avg_Prec did not improve from 0.42352\n",
      "Epoch 16/50\n",
      " - 1140s - loss: 0.0112 - acc: 0.9986 - val_loss: 0.0573 - val_acc: 0.9936\n",
      " epoch:15 AUC: 0.8471\n",
      " epoch:15 Avg_Prec: 0.4188\n",
      "\n",
      "Epoch 00016: Avg_Prec did not improve from 0.42352\n",
      "Epoch 17/50\n",
      " - 1152s - loss: 0.0104 - acc: 0.9988 - val_loss: 0.0551 - val_acc: 0.9932\n",
      " epoch:16 AUC: 0.8581\n",
      " epoch:16 Avg_Prec: 0.4502\n",
      "\n",
      "Epoch 00017: Avg_Prec improved from 0.42352 to 0.45025, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 18/50\n",
      " - 1134s - loss: 0.0101 - acc: 0.9987 - val_loss: 0.0542 - val_acc: 0.9927\n",
      " epoch:17 AUC: 0.8559\n",
      " epoch:17 Avg_Prec: 0.4361\n",
      "\n",
      "Epoch 00018: Avg_Prec did not improve from 0.45025\n",
      "Epoch 19/50\n",
      " - 1148s - loss: 0.0098 - acc: 0.9990 - val_loss: 0.0506 - val_acc: 0.9932\n",
      " epoch:18 AUC: 0.8589\n",
      " epoch:18 Avg_Prec: 0.4831\n",
      "\n",
      "Epoch 00019: Avg_Prec improved from 0.45025 to 0.48309, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 20/50\n",
      " - 1150s - loss: 0.0100 - acc: 0.9989 - val_loss: 0.0587 - val_acc: 0.9931\n",
      " epoch:19 AUC: 0.8659\n",
      " epoch:19 Avg_Prec: 0.4102\n",
      "\n",
      "Epoch 00020: Avg_Prec did not improve from 0.48309\n",
      "Epoch 21/50\n",
      " - 1153s - loss: 0.0098 - acc: 0.9989 - val_loss: 0.0527 - val_acc: 0.9934\n",
      " epoch:20 AUC: 0.8747\n",
      " epoch:20 Avg_Prec: 0.4884\n",
      "\n",
      "Epoch 00021: Avg_Prec improved from 0.48309 to 0.48838, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 22/50\n",
      " - 1140s - loss: 0.0096 - acc: 0.9990 - val_loss: 0.0553 - val_acc: 0.9928\n",
      " epoch:21 AUC: 0.8552\n",
      " epoch:21 Avg_Prec: 0.4487\n",
      "\n",
      "Epoch 00022: Avg_Prec did not improve from 0.48838\n",
      "Epoch 23/50\n",
      " - 1146s - loss: 0.0094 - acc: 0.9991 - val_loss: 0.0552 - val_acc: 0.9929\n",
      " epoch:22 AUC: 0.8727\n",
      " epoch:22 Avg_Prec: 0.4509\n",
      "\n",
      "Epoch 00023: Avg_Prec did not improve from 0.48838\n",
      "Epoch 24/50\n",
      " - 1139s - loss: 0.0089 - acc: 0.9990 - val_loss: 0.0521 - val_acc: 0.9929\n",
      " epoch:23 AUC: 0.8903\n",
      " epoch:23 Avg_Prec: 0.4887\n",
      "\n",
      "Epoch 00024: Avg_Prec improved from 0.48838 to 0.48873, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 25/50\n",
      " - 1142s - loss: 0.0097 - acc: 0.9988 - val_loss: 0.0510 - val_acc: 0.9933\n",
      " epoch:24 AUC: 0.8752\n",
      " epoch:24 Avg_Prec: 0.4654\n",
      "\n",
      "Epoch 00025: Avg_Prec did not improve from 0.48873\n",
      "Epoch 26/50\n",
      " - 1135s - loss: 0.0095 - acc: 0.9992 - val_loss: 0.0555 - val_acc: 0.9917\n",
      " epoch:25 AUC: 0.8713\n",
      " epoch:25 Avg_Prec: 0.4852\n",
      "\n",
      "Epoch 00026: Avg_Prec did not improve from 0.48873\n",
      "Epoch 27/50\n",
      " - 1143s - loss: 0.0094 - acc: 0.9991 - val_loss: 0.0574 - val_acc: 0.9925\n",
      " epoch:26 AUC: 0.8724\n",
      " epoch:26 Avg_Prec: 0.4393\n",
      "\n",
      "Epoch 00027: Avg_Prec did not improve from 0.48873\n",
      "Epoch 28/50\n",
      " - 1131s - loss: 0.0087 - acc: 0.9993 - val_loss: 0.0631 - val_acc: 0.9926\n",
      " epoch:27 AUC: 0.8589\n",
      " epoch:27 Avg_Prec: 0.3923\n",
      "\n",
      "Epoch 00028: Avg_Prec did not improve from 0.48873\n",
      "Epoch 29/50\n",
      " - 1147s - loss: 0.0097 - acc: 0.9989 - val_loss: 0.0576 - val_acc: 0.9912\n",
      " epoch:28 AUC: 0.8738\n",
      " epoch:28 Avg_Prec: 0.4941\n",
      "\n",
      "Epoch 00029: Avg_Prec improved from 0.48873 to 0.49413, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 30/50\n",
      " - 1153s - loss: 0.0096 - acc: 0.9990 - val_loss: 0.0505 - val_acc: 0.9935\n",
      " epoch:29 AUC: 0.8787\n",
      " epoch:29 Avg_Prec: 0.5096\n",
      "\n",
      "Epoch 00030: Avg_Prec improved from 0.49413 to 0.50960, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200212\n",
      "Epoch 31/50\n",
      " - 1145s - loss: 0.0088 - acc: 0.9992 - val_loss: 0.0503 - val_acc: 0.9932\n",
      " epoch:30 AUC: 0.8782\n",
      " epoch:30 Avg_Prec: 0.4922\n",
      "\n",
      "Epoch 00031: Avg_Prec did not improve from 0.50960\n",
      "Epoch 32/50\n",
      " - 1146s - loss: 0.0097 - acc: 0.9990 - val_loss: 0.0577 - val_acc: 0.9934\n",
      " epoch:31 AUC: 0.8435\n",
      " epoch:31 Avg_Prec: 0.4502\n",
      "\n",
      "Epoch 00032: Avg_Prec did not improve from 0.50960\n",
      "Epoch 33/50\n",
      " - 1135s - loss: 0.0093 - acc: 0.9991 - val_loss: 0.0542 - val_acc: 0.9933\n",
      " epoch:32 AUC: 0.8563\n",
      " epoch:32 Avg_Prec: 0.4708\n",
      "\n",
      "Epoch 00033: Avg_Prec did not improve from 0.50960\n",
      "Epoch 34/50\n",
      " - 1137s - loss: 0.0091 - acc: 0.9991 - val_loss: 0.0573 - val_acc: 0.9925\n",
      " epoch:33 AUC: 0.8425\n",
      " epoch:33 Avg_Prec: 0.4559\n",
      "\n",
      "Epoch 00034: Avg_Prec did not improve from 0.50960\n",
      "Epoch 35/50\n",
      " - 1145s - loss: 0.0098 - acc: 0.9991 - val_loss: 0.0526 - val_acc: 0.9938\n",
      " epoch:34 AUC: 0.8499\n",
      " epoch:34 Avg_Prec: 0.4758\n",
      "\n",
      "Epoch 00035: Avg_Prec did not improve from 0.50960\n",
      "Epoch 00035: early stopping\n",
      "training end...\n"
     ]
    }
   ],
   "source": [
    "basicmodel = basicModel(embedding_matrix,MAX_NB_WORDS,MAX_PARA_LENGTH)\n",
    "training = trainModel(X_train,y_train,ModelName, basicmodel, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicmodel.load_weights(ModelName)\n",
    "pred = basicmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_test = average_precision_score(y_test, pred)\n",
    "ap_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_test = roc_auc_score(y_test, pred)\n",
    "auc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
