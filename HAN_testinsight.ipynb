{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import _pickle as pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import operator\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Dropout, Activation, Input, Flatten, Concatenate, GlobalAveragePooling1D, LSTM, GRU, Bidirectional, dot, multiply, Lambda, TimeDistributed, Masking\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2,l1,l1_l2\n",
    "from keras.callbacks import Callback,EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, precision_recall_curve, classification_report,accuracy_score, auc, roc_curve, roc_auc_score, average_precision_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    f = open(data_name,'rb')\n",
    "    data = pickle.load(f)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARA_LENGTH = 250\n",
    "MAX_PARAS = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "#VALIDATION_SPLIT = 0.2\n",
    "#lr = 0.0005\n",
    "opt = optimizers.Adam(lr= 0.0005)\n",
    "#l1_reg = l1(1e-5)\n",
    "l12_reg = l1_l2(l1 = 1e-6,l2 = 1e-6)\n",
    "BATCH_SIZE = 24\n",
    "#metrics_auc = {}\n",
    "metrics_prcs = {}\n",
    "epochs_num = 35\n",
    "#num_folder = 5\n",
    "path = '/home/jujun/fraudprediction_10k/data/'\n",
    "hanpath = '/home/jujun/fraudprediction_10k/HAN/cp-{epoch:04d}.ckpt'\n",
    "date = '20200225'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (56288, 200, 250)\n",
      "embeddng matrix shape: (50001, 100)\n",
      "45030\n",
      "11258\n",
      "45030\n",
      "11258\n",
      "(45030, 200, 250)\n",
      "(11258, 200, 250)\n",
      "y train: 0    44625\n",
      "1      405\n",
      "dtype: int64\n",
      "y test: 0    11157\n",
      "1      101\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with open(path + 'word_index_50000_20200215','rb') as fp:\n",
    "    word_index_50000 = pickle.load(fp)\n",
    "\n",
    "type(word_index_50000)\n",
    "\n",
    "\n",
    "data = np.load(path + 'handata_20200215.npy')\n",
    "print(\"data shape:\", data.shape)\n",
    "\n",
    "embedding_matrix = np.load(path +'embedding_matrix_20200215.npy')\n",
    "print(\"embeddng matrix shape:\", embedding_matrix.shape)\n",
    "\n",
    "\n",
    "train_labels = load_data(path +'y_train_20200214')\n",
    "print(len(train_labels))\n",
    "\n",
    "test_labels = load_data(path + 'y_test_20200214')\n",
    "print(len(test_labels))\n",
    "\n",
    "\n",
    "train_indecis = load_data(path + 'indices_train_20200214')\n",
    "print(len(train_indecis))\n",
    "\n",
    "test_indecis = load_data(path + 'indices_test_20200214')\n",
    "print(len(test_indecis))\n",
    "\n",
    "X_train = data[train_indecis]\n",
    "X_test = data[test_indecis]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "ModelName = hanpath + 'hanmodel_l1l2_drpt' + date \n",
    "\n",
    "y_train = pd.Series(train_labels)\n",
    "\n",
    "y_train.shape\n",
    "print(\"y train:\", y_train.value_counts())\n",
    "\n",
    "y_test= pd.Series(test_labels)\n",
    "print(\"y test:\", y_test.value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(value):\n",
    "    return {k:v for k,v in word_index_50000.items() if v == value}\n",
    "\n",
    "\n",
    "def performance_measure(pred_yp, y):\n",
    "    '''\n",
    "    Given lists of predicted y probability and x, y, return a dataframe of AR, AUC, Brier, Decile Table\n",
    "    '''\n",
    "    \n",
    "    tenc_dat = pd.DataFrame({'y_true':y,'probability':pred_yp.flatten()})\n",
    "    tenc_dat.sort_values('probability',axis = 0,ascending=False, inplace = True)\n",
    "    tenc_dat.index = range(0,len(tenc_dat))\n",
    "    y = tenc_dat['y_true']\n",
    "    point = float(len(tenc_dat))/10\n",
    "    point = int(round(point))\n",
    "    tenc = []\n",
    "    for i in range(0,10):\n",
    "        tenc.append(y[(i*point):((i+1)*point)])\n",
    "    tenc[9]=tenc[9].append(y[10*point:])\n",
    "    total = sum(y)\n",
    "    num_of_bkr = []\n",
    "    for j in range(0,10):\n",
    "        num_of_bkr.append(sum(tenc[j]))\n",
    "    tencile_bkr = np.array(num_of_bkr)\n",
    "    rate = tencile_bkr.astype(float)/total\n",
    "\n",
    "    return rate\n",
    "\n",
    "\n",
    "class Evaluation(Callback):\n",
    "    \"\"\" Show AUC after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logs['auc'] = score\n",
    "            score_ap = average_precision_score(self.y_val, y_pred)\n",
    "            logs['Avg_Prec'] = score_ap\n",
    "            #tencile=performance_measure(y_pred, self.y_val)\n",
    "            #logs['tencile'] = tencile\n",
    "            print(\" epoch:{:d} AUC: {:.4f}\".format(epoch, score))\n",
    "            print(\" epoch:{:d} Avg_Prec: {:.4f}\".format(epoch, score_ap))\n",
    "\n",
    "\n",
    "# class PrecisionEvaluation(Callback):\n",
    "#     \"\"\" Show average Precision after interval number of epoches \"\"\"\n",
    "#     def __init__(self, validation_data=(), interval=1):\n",
    "#         super(Callback, self).__init__()\n",
    "#         self.interval = interval\n",
    "#         self.X_val, self.y_val = validation_data\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         if epoch % self.interval == 0:\n",
    "#             y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "#             score = average_precision_score(self.y_val, y_pred)\n",
    "#             logs['Avg_Prec'] = score\n",
    "#             #tencile=performance_measure(y_pred, self.y_val)\n",
    "#             #logs['tencile'] = tencile\n",
    "#             #print('prec_socre',score)\n",
    "#             print(\" epoch:{:d} Avg_Prec: {:.4f}\".format(epoch, score))\n",
    "\n",
    "\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    \n",
    "    def __init__(self, regularizer=None,context_dim=100, name=\"attention\",**kwargs):\n",
    "        self.regularizer = regularizer\n",
    "        self.context_dim=context_dim\n",
    "        self.supports_masking = True\n",
    "        self.name=name\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3        \n",
    "        self.W = self.add_weight(name='W', shape=(input_shape[-1], self.context_dim), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.b = self.add_weight(name='b', shape=(self.context_dim,), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.u = self.add_weight(name='u', shape=(self.context_dim,), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)        \n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        eij = K.squeeze(K.dot(eij, K.expand_dims(self.u, axis=1)), axis=-1)\n",
    "        ai = K.exp(eij)\n",
    "\n",
    "        \n",
    "        if mask is not None:\n",
    "            ai*=K.cast(mask, K.floatx())\n",
    "            \n",
    "        ai /=K.cast(K.sum(ai, axis=1, keepdims=True)+K.epsilon(), K.floatx())\n",
    "\n",
    "        return ai\n",
    "        \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(AttLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class WeightedSum(Layer):\n",
    "    def __init__(self, name=\"weighted_sum\",  **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.name=name\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(WeightedSum, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        \n",
    "        x = input_tensor[0]\n",
    "        #print(\"input\",K.int_shape(x))\n",
    "        a = input_tensor[1]\n",
    "        print(\"weights\", K.int_shape(a))\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = K.sum(x * a, axis=1)\n",
    "        #print(\"weighted sum\",K.int_shape(weighted_input))\n",
    "        \n",
    "        return weighted_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        a, b = input_shape\n",
    "        \n",
    "        return (a[0], a[-1])\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def basicModel(embedding_matrix,MAX_NB_WORDS,MAX_PARA_LENGTH,):\n",
    "    embedding_layer = Embedding((MAX_NB_WORDS+ 1),\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            mask_zero=True,\n",
    "                            input_length=MAX_PARA_LENGTH,\n",
    "                            trainable=True)    \n",
    "    \n",
    "    \n",
    "    para_input = Input(shape=(MAX_PARA_LENGTH, ), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(para_input)\n",
    "    norm_sequence = BatchNormalization()(embedded_sequences)\n",
    "    drop_out = Dropout(0.2)(norm_sequence)\n",
    "    l_att = AttLayer(regularizer = l12_reg)(drop_out)\n",
    "    weighted_sum = WeightedSum()([drop_out,l_att])\n",
    "    paraEncoder =Model(para_input,weighted_sum)\n",
    "    paraEncoder.summary()\n",
    "    \n",
    "    doc_input = Input(shape=(MAX_PARAS, MAX_PARA_LENGTH), dtype='int32')\n",
    "    doc_encoder = TimeDistributed(paraEncoder)(doc_input)\n",
    "    mask_doc = Masking(mask_value=0.0)(doc_encoder)\n",
    "    norm_doc = BatchNormalization()(mask_doc)\n",
    "    l_lstm_para = LSTM(100, return_sequences=True,\\\n",
    "                      implementation=2, \\\n",
    "                      recurrent_dropout = 0.2,\n",
    "                      dropout=0.2)(norm_doc)\n",
    "    norm_doc_1 = BatchNormalization()(l_lstm_para)\n",
    "    l_att_para = AttLayer(regularizer=l12_reg)(norm_doc_1)\n",
    "    weighted_sum_doc = WeightedSum()([norm_doc_1, l_att_para])\n",
    "\n",
    "    batch_norm = BatchNormalization()(weighted_sum_doc)\n",
    "\n",
    "    drop_out = Dropout(0.2)(batch_norm)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid',kernel_regularizer=l12_reg)(drop_out) \n",
    "\n",
    "    model = Model(doc_input, preds)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trainModel(x_train, y_train, Model_Filepath, model):\n",
    "    \n",
    "    \n",
    "    #class_weights = class_weight.compute_class_weight('balanced',\n",
    "    #                                             np.unique(y_train),\n",
    "    #                                             y_train)\n",
    "    #print(\"class weights:\", class_weights)\n",
    "    \n",
    "    class_weights = {0: 1, 1:10.0}\n",
    "    print(\"class weights:\", class_weights)\n",
    "    \n",
    "#     val_class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                              np.unique(y_val),\n",
    "#                                              y_val)\n",
    "#     print(\"validation class weights:\", val_class_weights)\n",
    "    \n",
    "#     val_sample_weights = []\n",
    "#     for y in y_val:\n",
    "#         if y == 1:\n",
    "#             val_sample_weights.append(val_class_weights[1])\n",
    "#         else: val_sample_weights.append(val_class_weights[0])\n",
    "#     val_sample_weights = np.asarray(val_sample_weights)\n",
    "    \n",
    "#    print(val_sample_weights[0:10])\n",
    "    \n",
    "#    print(\"shape of weights: \",val_sample_weights.shape, \"shape y_val: \", y_val.shape)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer= opt,\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    auc_ap_eval = Evaluation(validation_data=(x_train, y_train), interval=1)\n",
    "    #precision_eval = PrecisionEvaluation(validation_data=(x_train, y_train), interval=1)\n",
    "    \n",
    "    #earlyStopping = EarlyStopping(monitor='Avg_Prec',patience = 5, verbose =2, mode ='max')\n",
    "    checkpoint = ModelCheckpoint(Model_Filepath,save_weights_only=True, period=5)\n",
    "    #monitor='Avg_Prec', verbose=2, save_best_only=True, mode ='max')\n",
    "                                 \n",
    "    print(\"training start...\")\n",
    "    training=model.fit(x_train,y_train,epochs=epochs_num,batch_size=BATCH_SIZE,callbacks=[auc_ap_eval,checkpoint],\n",
    "                           class_weight = class_weights,verbose=2) #validation_data=[x_val,y_val,val_sample_weights],earlyStopping,\n",
    "      \n",
    "#     hist_data = list(zip(training.history['acc'],\\\n",
    "#                                  training.history['loss'],\\\n",
    "#                                  training.history['val_acc'],\\\n",
    "#                                  training.history['val_loss'],\\\n",
    "#                                  training.history['auc'],\n",
    "#                                  training.history['Avg_Prec']))\n",
    "#     hist_data=pd.DataFrame(hist_data, \\\n",
    "#                                    index = range(len(training.history['acc'])),\n",
    "#                                    columns =['acc','loss','val_acc','val_loss','auc','Avg_Prec'])\n",
    "                                 \n",
    "                                 \n",
    "#     hist_data[['acc','val_acc','Avg_Prec','auc']].plot.line()\n",
    "#     plt.savefig('plot1.pdf') \n",
    "#     hist_data[['loss','val_loss']].plot.line()\n",
    "#     plt.savefig('plot2.pdf')\n",
    "#     plt.close()\n",
    "            \n",
    "    \n",
    "    print('training end...')\n",
    "    \n",
    "    return training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights (None, 250)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 250, 100)     5000100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 250, 100)     400         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 250, 100)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_1 (AttLayer)          (None, 250)          10200       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "weighted_sum_1 (WeightedSum)    (None, 100)          0           dropout_1[0][0]                  \n",
      "                                                                 att_layer_1[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 5,010,700\n",
      "Trainable params: 5,010,500\n",
      "Non-trainable params: 200\n",
      "__________________________________________________________________________________________________\n",
      "weights (None, 250)\n",
      "weights (None, 200)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 200, 250)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 200, 100)     5010700     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 200, 100)     0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200, 100)     400         masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 200, 100)     80400       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200, 100)     400         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_2 (AttLayer)          (None, 200)          10200       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "weighted_sum_2 (WeightedSum)    (None, 100)          0           batch_normalization_3[0][0]      \n",
      "                                                                 att_layer_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         weighted_sum_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            101         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,102,601\n",
      "Trainable params: 5,101,801\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "basicmodel = basicModel(embedding_matrix,MAX_NB_WORDS,MAX_PARA_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicmodel.load_weights(\"/home/jujun/fraudprediction_10k/HAN/cp-0020.ckpthanmodel_l1l2_drpt20200225\")\n",
    "pred = basicmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tb = pd.read_csv('/home/jujun/fraudprediction_10k/data/test_insight_20200214.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tb['pred_prob'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>cik</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edgar/data/815910/0000815910-04-000041.txt</td>\n",
       "      <td>815910</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>edgar/data/1294538/0000950124-05-001833.txt</td>\n",
       "      <td>1294538</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>edgar/data/49401/0000049401-05-000016.txt</td>\n",
       "      <td>49401</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>edgar/data/1326973/0000950134-07-023627.txt</td>\n",
       "      <td>1326973</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>edgar/data/1043000/0000950134-06-006443.txt</td>\n",
       "      <td>1043000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          file      cik  label  pred_prob\n",
       "0   edgar/data/815910/0000815910-04-000041.txt   815910      0   0.000140\n",
       "1  edgar/data/1294538/0000950124-05-001833.txt  1294538      0   0.000015\n",
       "2    edgar/data/49401/0000049401-05-000016.txt    49401      0   0.000259\n",
       "3  edgar/data/1326973/0000950134-07-023627.txt  1326973      0   0.000212\n",
       "4  edgar/data/1043000/0000950134-06-006443.txt  1043000      0   0.000032"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tb_fraud = test_tb[test_tb.label == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_fraud = list(set(test_tb_fraud.cik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tb_fraud = test_tb[test_tb.cik.isin(cik_fraud)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvtb_fraud_han = pd.pivot_table(test_tb_fraud, values='pred_prob', index=['cik'], columns =['label'],\n",
    "                     aggfunc=[np.average,len], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvtb_fraud_han_ave_1 = pvtb_fraud_han['average'][1].rename('prob_ave')\n",
    "\n",
    "pvtb_fraud_han_len_1 = pvtb_fraud_han['len'][1].rename('count_lb_1')\n",
    "\n",
    "tb_lb_1 = pd.concat([pvtb_fraud_han_ave_1, pvtb_fraud_han_len_1],axis=1)\n",
    "\n",
    "tb_lb_1 = tb_lb_1.drop('All',axis =0)\n",
    "\n",
    "tb_lb_1 = tb_lb_1.sort_values(by=['count_lb_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEtlJREFUeJzt3W+MXFd9xvHn8Xpd6kJB2NsKxfZupBqpdtoCGkWp6AsXTOvkhfOiFYoZShsQqyqkCoIiha4ENJJVtUi0QQqBlcChaEnq0lJZ1G2K0iCkqkkzLhCwQ9Aq9RK7SNk4kBathP/9+uLO1rPr2Z0z7Ozce+58P9Jq9p453jny3Xnm7LnnnuOIEACgXraU3QAAwOAR7gBQQ4Q7ANQQ4Q4ANUS4A0ANEe4AUEOEOwDUEOEOADVEuANADW0t64V37twZU1NTZb08AGTp1KlTL0bERK96pYX71NSUWq1WWS8PAFmyvZBSj2EZAKghwh0AaohwB4AaItwBoIYIdwCooZ7hbvtztl+w/Z01nrftT9qet/207TcNvpltBw9K9rWvgwc37aXQh7vukrZuLc7J1q3FMYBSpfTcH5J0aJ3nb5W0t/01LenBjTeri4MHpcceW1n22GMEfNnuukt68EHpypXi+MqV4piAB0rllG32bE9J+kpE3NTluc9I+lpEPNw+flbSgYj4wXo/s9FoRF/z3O21n2OrwPJs3Xot2DuNjUmXLw+/PUDN2T4VEY1e9QYx5n6DpOc7js+1y7o1atp2y3ZrcXFxAC+N0nUL9vXKAQzFUC+oRsRsRDQiojEx0fPuWeRgbKy/cgBDMYhwPy9pd8fxrnYZRsGBA/2VAxiKQYT7CUnvas+auUXSy73G21Ej8/P9lQMYip4Lh9l+WNIBSTttn5P0UUnjkhQRn5Z0UtJtkuYlLUm6c7Maiwr6/vf7KwcwFD3DPSKO9Hg+JL1vYC1ai919Vsx6s2iw+fbskRa6LFK3Z8/w2wLg/+Vzh+pb3tJfOYbj6FFp+/aVZdu3F+UASpNPuDO2W03NpjQ7K01OFn9FTU4Wx81m2S0DRlrSTUybgZuYAKB/w7yJCQBQMYQ7Nm5uTpqakrZsKR7n5spuETDySttDtW9jY2uvYYLyzM1J7363dPFicbywUBxLjLsDJcqn5z493V85huOee64F+7KLF4tyAKXJp+f+qU8Vj7OzRQ9+bKwI9uVylOPChf7KAQxFPuEuFUFOmANAT/kMy6CaduzorxzAUOQV7szKqJ7775fGx1eWjY8X5QBKk0+4z80VY+wLC8VNSwsLxTEBX65mUzp2bOUdqseOMVMGKFk+d6hOTXVfoGpyUjp7dlDNAoBKq98dqiwtW10MlwGVk0+4r7WELEvLlovhMqCS8gl3lpatppkZaWlpZdnSUlEOoDT5hDtLy1YTw2VAJeV1E1OzSZhXDTsxAZWUT88d1cRwGVBJhDs2huEyoJLyGpZBNTFcBlROXj135lMDQJJ8eu7L86mXp90tz6eW6DUCwCr59NyZTw0AyfIJd+ZTA0CyfMKd5QcAIFk+4X70qLRt28qybduYTw0AXeQT7lKxMNV6xwAASTmF+8yMdOnSyrJLl7igWgVMUQUqJ59w77Z+yXrlGA6W/K0mPnBHXlK42z5k+1nb87bv7fL8HtuP2/6G7adt3zb4pqKSmKJaPXzgQgnb7Nkek/Q9SW+TdE7SU5KORMSZjjqzkr4REQ/a3ifpZERMrfdz+95mz177Ocbey7NlS/f/f1u6enX47QFbUtbcILfZu1nSfEQ8FxEXJT0i6fZVdULSz7e/f7Wk/+6nscgYU1Srh3tCoLRwv0HS8x3H59plnT4m6Z22z0k6KemPBtI6VB9L/lYPH7jQ4C6oHpH0UETsknSbpC/Yvu5n25623bLdWlxc7O8VJif7K8dwsORv9fCBC6WF+3lJuzuOd7XLOr1H0nFJioh/l/QKSTtX/6CImI2IRkQ0JiYm+mspNzFVV7NZjOVevVo8Euzl4gMXSlsV8ilJe23fqCLU75D0jlV1vi/prZIesv3LKsK9z655Am5iAtKwxv7I69lzj4jLku6W9KikZyQdj4jTtu+zfbhd7YOS3mv7W5IelvQH0WsaTr+4iQkAkvWcCrlZ+p4KyZQ7ABjoVMhqYAYAACTLJ9yZAQAAyfIJd2YAAECyfPZQlZgBAACJ8um5S6x0V1Wcl+rhnIy8fHruyyvdLa9AuLzSnURvvkycl+rhnEA5TYVkpbtq4rxUD+ek1uo3FZLNOqqJFQirh3MC5RTuY2P9lWM4uP+gejgnUE7hfuVKf+UYDu4/qB7OCZRTuLPkbzVx/0H1cE6gnC6ozs1Jd965cvGw8XHp2DF+aQGMjPpdUJWu30d1vX1VAWCE5RPuMzPSxYsryy5eZMlfAOgin3BnehcAJMsn3JneBQDJ8gl3pncBQLJ8wp3pXQCQLJ+FwySW/AWARPn03AEAyQh3AKghwh0AaohwB4AaItwBoIYIdwCoIcIdAGqIcAeAGsor3Ofmis1/t2wpHufmym4RAFRSPneozs1J09PS0lJxvLBQHEvctQoAq+TTc5+ZuRbsy5aWWM8dALrIJ9xZzx0AkiWFu+1Dtp+1PW/73jXqvN32GdunbX9xsM0U67kDQB96hrvtMUkPSLpV0j5JR2zvW1Vnr6QPS3pzROyX9P6Bt5T13AEgWUrP/WZJ8xHxXERclPSIpNtX1XmvpAci4oeSFBEvDLaZYj13AOhDSrjfIOn5juNz7bJOr5f0etv/ZvsJ24e6/SDb07ZbtluLi4v9t7bZlM6ela5eLR4JdgA5GeJ07kFNhdwqaa+kA5J2Sfq67V+JiB91VoqIWUmzktRoNGJArw0A1Tfk6dwpPffzknZ3HO9ql3U6J+lERFyKiP+S9D0VYQ8AkIY+nTsl3J+StNf2jba3SbpD0olVdf5BRa9dtneqGKZ5boDtBIC8DXk6d89wj4jLku6W9KikZyQdj4jTtu+zfbhd7VFJF2yfkfS4pA9FxIVNaTEA5GjI07kdUc7Qd6PRiFarVcprA8DQrR5zl4rp3H3O+rN9KiIaverlc4cqAORsyNO581k4DABy12wObQo3PXcAqCHCHQBqiHAHgBoi3AGghgh3AKghwh0AaohwB4AaItwBoIYIdwCoIcIdGzfEDQgApGH5AWzMkDcgAJCGnjs2ZsgbEABIQ7hjY4a8AQGANIQ7NmbIGxAASEO4Y2OOHi02HOi0fXtRDqA0hDs2ZsgbEABIk1e4M+WumppN6exZ6erV4pFgB0qXz1RIptwBQLJ8eu5MuQOAZPmEO1PuACBZPuHOlDsASJZPuDPlDgCS5RPuTLkDgGT5hDuAdEwbHnlMhQTqhvcKJDkiSnnhRqMRrVYr/R9MTRW/pKtNThY3zgAo8F6pNdunIqLRq14+wzJMhQTS8F6Bcgp3pkICaXivQInhbvuQ7Wdtz9u+d516v2M7bPf8k6FvTIUE0vBegRLC3faYpAck3Sppn6Qjtvd1qfcqSfdIenLQjZTEVEggFe8VKOGCqu1fl/SxiPjt9vGHJSki/mxVvb+S9FVJH5L0xxGx7tXSvi+oAgAGekH1BknPdxyfa5d1vtibJO2OiH/sq5UAgE2x4QuqtrdI+oSkDybUnbbdst1aXFzc6EsDANaQEu7nJe3uON7VLlv2Kkk3Sfqa7bOSbpF0ottF1YiYjYhGRDQmJiZ++lYDANaVEu5PSdpr+0bb2yTdIenE8pMR8XJE7IyIqYiYkvSEpMO9xtwBAJunZ7hHxGVJd0t6VNIzko5HxGnb99k+vNkNBAD0L2ltmYg4KenkqrKPrFH3wMabBQDYiHzuUAUAJCPcAaCGCHcAqCHCHQBqiHAHgBoi3AGghvIKd/aFBNLwXhl5ee2heued0qVLxfHCQnEssZQp0Ik9VKGc9lDduVO6cOH68h07pBdfHFzDgNyxh2qt1W8P1W7Bvl45MKrYQxXKKdwBpGEPVSincN+xo79yYFSxhyqUU7jff7+0bdvKsm3binIA17CHKpTTbJnlX8yZmWLscM+eoifCLyxwvWaT98aIyyfcJX5hASBRPsMyAIBkhDsA1BDhDgA1RLgDQA0R7gBQQ4Q7ANQQ4Q4ANZRXuLNGNQAkyecmJtaoBoBk+fTcZ2auBfuypaWiHACwQj7hzhrVAJAsn3BnjWoASJZPuLNGNQAkyyfcWaMaAJLlM1tGYslfAEiUT88dAJAsKdxtH7L9rO152/d2ef4Dts/Yftr2Y7YnB99UAECqnuFue0zSA5JulbRP0hHb+1ZV+4akRkT8qqQvSfqLQTcUAJAuped+s6T5iHguIi5KekTS7Z0VIuLxiFi+w+gJSbsG28w2lh8AgCQp4X6DpOc7js+1y9byHkn/1O0J29O2W7Zbi4uL6a2Uri0/sLAgRVxbfoCAB4DrDPSCqu13SmpI+ni35yNiNiIaEdGYmJjo74ez/AAAJEsJ9/OSdncc72qXrWD7oKQZSYcj4ieDaV6HhYX+yjE8DJcBlZMyz/0pSXtt36gi1O+Q9I7OCrbfKOkzkg5FxAsDb6UkjY1JV650L0d5WK0TqKSePfeIuCzpbkmPSnpG0vGIOG37PtuH29U+LumVkv7W9jdtnxh4S7sF+3rlGA6Gy4BKSrpDNSJOSjq5quwjHd8fHHC7rjc52X0IZpIp9aVitU6gkvK5Q5WFw6qJ1TqBSson3Fk4rJr40AUqiYXDsDHL52NmphiK2bOnCHbOE1CqfHruElPuqqrZlM6ela5eLR4JdqB0+fTcmXIHAMny6bkz5Q4AkuUT7ky5A4Bk+YT7a1/bXzkAjLB8wh0AkCyfcH/ppf7KAWCE5RPuDMsAQLJ8wv0na6wivFY5AIywfML9xz/urxwARlg+4Q4ASJZPuO/Y0V85AIywfML97W/vrxwARlg+4X7yZH/lADDC8gl3lh8AgGT5hDs7/gBAsnzC/bbb+isHgBGWT7gfP95fOQCMsHzC/cKF/soBYITlE+4AgGT5hDs3MQFAsnzC/f77pfHxlWXj40U5AGCFfMK92ZSOHZMmJyW7eDx2jM2xAaCLrWU3oC/NJmEOAAny6bkDAJLlFe779xdDMstf+/eX3SIAqKR8wn3/funMmZVlZ84Q8ADQRVK42z5k+1nb87bv7fL8z9j+m/bzT9qeGnRDrwv2XuUAMMJ6hrvtMUkPSLpV0j5JR2zvW1XtPZJ+GBG/JOkvJf35oBuKCpubk6ampC1bise5ubJbBIy8lJ77zZLmI+K5iLgo6RFJt6+qc7ukz7e//5Kkt9r24JqJypqbk6anpYUFKaJ4nJ4m4IGSpYT7DZKe7zg+1y7rWiciLkt6WRK3jo6CmRlpaWll2dJSUQ6gNEO9oGp72nbLdmtxcXGYL43NwiYqQCWlhPt5Sbs7jne1y7rWsb1V0qslXbdcY0TMRkQjIhoTExM/XYtRLWyiAlRSSrg/JWmv7Rttb5N0h6QTq+qckPT77e9/V9K/RkQMrpmorKNHpe3bV5Zt316UAyhNz3Bvj6HfLelRSc9IOh4Rp23fZ/twu9pnJe2wPS/pA5Kumy65YWt9VvAZUq5mU5qdXbnmz+wsy0QAJXNZHexGoxGtVquU1waAXNk+FRGNXvXyuUMVAJCMcAeAGiLcAaCGCHcAqCHCHQBqqLTZMrYXJS38lP98p6QXB9gcDAbnpXo4J9W0kfMyGRE97wItLdw3wnYrZSoQhovzUj2ck2oaxnlhWAYAaohwB4AayjXcZ8tuALrivFQP56SaNv28ZDnmDgBYX649dwDAOiob7rY/Z/sF299Z43nb/mR7U+6nbb9p2G0cRbZ3237c9hnbp23f06UO52aIbL/C9n/Y/lb7nPxplzqbv4k9rmP7rO1v2/6m7etWStzM90plw13SQ5IOrfP8rZL2tr+mJT04hDZBuizpgxGxT9Itkt7XZcN0zs1w/UTSWyLi1yS9QdIh27esqsMm9uX5zYh4wxpTHzftvVLZcI+Ir0t6aZ0qt0v66yg8Iek1tl83nNaNroj4QUT8Z/v7/1Wxxv/qPXU5N0PU/n/+cftwvP21+mIam9hX06a9Vyob7glSNu7GJmr/af9GSU+ueopzM2S2x2x/U9ILkr4aEWueEzaxH6qQ9C+2T9me7vL8pr1Xtg7ih2D02H6lpL+T9P6I+J+y2zPqIuKKpDfYfo2kL9u+KSK6Xq/CUP1GRJy3/QuSvmr7u+1RiU2Xc889ZeNubALb4yqCfS4i/r5LFc5NSSLiR5Ie1/XXq5I2scdgRcT59uMLkr4s6eZVVTbtvZJzuJ+Q9K721eZbJL0cET8ou1F11x6n/aykZyLiE2tU49wMke2Jdo9dtn9W0tskfXdVNTaxHzLbP2f7VcvfS/otSav/mtq090plh2VsPyzpgKSdts9J+qiKC0WKiE9LOinpNknzkpYk3VlOS0fOmyX9nqRvt8d4JelPJO2RODcleZ2kz9seU9FhOx4RX7F9n6RWRJxQ8YH8hfYm9i9JuqO85o6MX1QxRCYVWfvFiPhn238obf57hTtUAaCGch6WAQCsgXAHgBoi3AGghgh3AKghwh0AaohwB4AaItwBoIYIdwCoof8DR4rELrrgn1oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = [str(i) for i in tb_lb_1.count_lb_1]\n",
    "plt.plot(x_axis, tb_lb_1.prob_ave,'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
