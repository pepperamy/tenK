{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import _pickle as pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import operator\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Dropout, Activation, Input, Flatten, Concatenate, GlobalAveragePooling1D, LSTM, GRU, Bidirectional, dot, multiply, Lambda, TimeDistributed, Masking\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2,l1,l1_l2\n",
    "from keras.callbacks import Callback,EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, precision_recall_curve, classification_report,accuracy_score, auc, roc_curve, roc_auc_score, average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    f = open(data_name,'rb')\n",
    "    data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('/home/jujun/fraudprediction_10k/data/data_20200212_2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_para = []\n",
    "#labels = []\n",
    "for d in data:\n",
    "    text_para.append(d[0])\n",
    "    #labels.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doc(x):\n",
    "    return ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_doc = []\n",
    "for d in text_para:\n",
    "    text_doc.append(generate_doc(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARA_LENGTH = 250\n",
    "MAX_PARAS = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "#l1_reg = l1(1e-5)\n",
    "#l12_reg = l1_l2(l1 = 1e-6,l2 = 1e-6)\n",
    "BATCH_SIZE = 24\n",
    "#metrics_auc = {}\n",
    "metrics_prcs = {}\n",
    "num_folder = 5\n",
    "#date = '20200208'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 201564 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_50000 = { k:v for (k,v) in word_index.items() if v<=50000 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jujun/fraudprediction_10k/data/word_index_50000_20200228','wb') as fp:\n",
    "    pickle.dump(word_index_50000,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/jujun/fraudprediction_10k/HAN/input_data/word_index_50000_20200207','rb') as fp:\n",
    "#     word_index_5000 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_index_50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(value):\n",
    "    return {k:v for k,v in word_index_50000.items() if v == value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((len(text_doc), MAX_PARAS, MAX_PARA_LENGTH), dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56288, 200, 250)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(text_para):\n",
    "    #structure of a doc: [p1,p2,p3,p4]\n",
    "    for j, para in enumerate(doc):\n",
    "        \n",
    "        if j < MAX_PARAS:\n",
    "            wordTokens = text_to_word_sequence(para)\n",
    "            \n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_PARA_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.load('data_20200206.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/jujun/fraudprediction_10k/data/handata_20200228', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_w2v = [x.split(' ') for x in text_doc]\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get my W2V model\n",
    "#Parameter: window=5,size=EMBEDDING_DIM, sg=1, workers=os.cpu_count(),min_count=5\n",
    "w2v_model = Word2Vec(doc_w2v, window=5, workers=os.cpu_count(), min_count=5, size=EMBEDDING_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros(((MAX_NB_WORDS+ 1) , EMBEDDING_DIM))\n",
    "ignored=[]\n",
    "cnt=0\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv.vocab and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "        embedding_vector = w2v_model[word]\n",
    "        cnt+=1\n",
    "        if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    else: ignored.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120152"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49334"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/jujun/fraudprediction_10k/data/embedding_matrix_20200228', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('/home/jujun/fraudprediction_10k/data/w2v_model_202002028.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_matrix = np.load('embedding_matrix_20200212.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measure(pred_yp, y):\n",
    "    '''\n",
    "    Given lists of predicted y probability and x, y, return a dataframe of AR, AUC, Brier, Decile Table\n",
    "    '''\n",
    "    \n",
    "    tenc_dat = pd.DataFrame({'y_true':y,'probability':pred_yp.flatten()})\n",
    "    tenc_dat.sort_values('probability',axis = 0,ascending=False, inplace = True)\n",
    "    tenc_dat.index = range(0,len(tenc_dat))\n",
    "    y = tenc_dat['y_true']\n",
    "    point = float(len(tenc_dat))/10\n",
    "    point = int(round(point))\n",
    "    tenc = []\n",
    "    for i in range(0,10):\n",
    "        tenc.append(y[(i*point):((i+1)*point)])\n",
    "    tenc[9]=tenc[9].append(y[10*point:])\n",
    "    total = sum(y)\n",
    "    num_of_bkr = []\n",
    "    for j in range(0,10):\n",
    "        num_of_bkr.append(sum(tenc[j]))\n",
    "    tencile_bkr = np.array(num_of_bkr)\n",
    "    rate = tencile_bkr.astype(float)/total\n",
    "\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUCEvaluation(Callback):\n",
    "    \"\"\" Show AUC after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logs['auc'] = score\n",
    "            tencile=performance_measure(y_pred, self.y_val)\n",
    "            logs['tencile'] = tencile\n",
    "            print(\" epoch:{:d} AUC: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionEvaluation(Callback):\n",
    "    \"\"\" Show average Precision after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = average_precision_score(self.y_val, y_pred)\n",
    "            logs['Avg_Prec'] = score\n",
    "            #tencile=performance_measure(y_pred, self.y_val)\n",
    "            #logs['tencile'] = tencile\n",
    "            #print('prec_socre',score)\n",
    "            print(\" epoch:{:d} Avg_Prec: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUCEvaluation(Callback):\n",
    "    \"\"\" Show AUC after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logs['auc'] = score\n",
    "            tencile=performance_measure(y_pred, self.y_val)\n",
    "            logs['tencile'] = tencile\n",
    "            print(\" epoch:{:d} AUC: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionEvaluation(Callback):\n",
    "    \"\"\" Show average Precision after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = average_precision_score(self.y_val, y_pred)\n",
    "            logs['Avg_Prec'] = score\n",
    "            #tencile=performance_measure(y_pred, self.y_val)\n",
    "            #logs['tencile'] = tencile\n",
    "            #print('prec_socre',score)\n",
    "            print(\" epoch:{:d} Avg_Prec: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    \n",
    "    def __init__(self, regularizer=None,context_dim=100, name=\"attention\",**kwargs):\n",
    "        self.regularizer = regularizer\n",
    "        self.context_dim=context_dim\n",
    "        self.supports_masking = True\n",
    "        self.name=name\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3        \n",
    "        self.W = self.add_weight(name='W', shape=(input_shape[-1], self.context_dim), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.b = self.add_weight(name='b', shape=(self.context_dim,), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.u = self.add_weight(name='u', shape=(self.context_dim,), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)        \n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        eij = K.squeeze(K.dot(eij, K.expand_dims(self.u, axis=1)), axis=-1)\n",
    "        #eij = K.dot(K.tanh(K.dot(x, self.W) + self.b), self.u)\n",
    "        ai = K.exp(eij)\n",
    "        #print(\"ai size\", K.int_shape(ai))\n",
    "        #print(\"mask size\", K.int_shape(mask))\n",
    "        \n",
    "        if mask is not None:\n",
    "            ai*=K.cast(mask, K.floatx())\n",
    "            \n",
    "        ai /=K.cast(K.sum(ai, axis=1, keepdims=True)+K.epsilon(), K.floatx())\n",
    "        #weights=K.expand_dims(weights, axis=2) # (None, 1000,1)\n",
    "        #alphas = ai / K.sum(ai, axis=1).dimshuffle(0, 'x')\n",
    "        \n",
    "        #weighted_input = x * K.expand_dims(ai)\n",
    "        #return K.sum(weighted_input, axis=1)\n",
    "        return ai\n",
    "        \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(AttLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(Layer):\n",
    "    def __init__(self, name=\"weighted_sum\",  **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.name=name\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #assert isinstance(input_shape, list)\n",
    "        #print(\"input shape\", input_shape)\n",
    "        super(WeightedSum, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        \n",
    "        x = input_tensor[0]\n",
    "        #print(\"input\",K.int_shape(x))\n",
    "        a = input_tensor[1]\n",
    "        print(\"weights\", K.int_shape(a))\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = K.sum(x * a, axis=1)\n",
    "        #print(\"weighted sum\",K.int_shape(weighted_input))\n",
    "        \n",
    "        return weighted_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        a, b = input_shape\n",
    "        \n",
    "        return (a[0], a[-1])\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicModel(embedding_matrix,MAX_NB_WORDS,MAX_PARA_LENGTH,):\n",
    "    embedding_layer = Embedding((MAX_NB_WORDS+ 1),\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            mask_zero=True,\n",
    "                            input_length=MAX_PARA_LENGTH,\n",
    "                            trainable=True)    \n",
    "    \n",
    "    \n",
    "    para_input = Input(shape=(MAX_PARA_LENGTH, ), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(para_input)\n",
    "    norm_sequence = BatchNormalization()(embedded_sequences)\n",
    "    drop_out = Dropout(0.2)(norm_sequence)     \n",
    "    l_att = AttLayer()(drop_out)\n",
    "    weighted_sum = WeightedSum()([norm_sequence,l_att])\n",
    "    paraEncoder =Model(para_input,weighted_sum)\n",
    "    paraEncoder.summary()\n",
    "    \n",
    "    doc_input = Input(shape=(MAX_PARAS, MAX_PARA_LENGTH), dtype='int32')\n",
    "    doc_encoder = TimeDistributed(paraEncoder)(doc_input)\n",
    "    mask_doc = Masking(mask_value=0.0)(doc_encoder)\n",
    "    norm_doc = BatchNormalization()(mask_doc)\n",
    "    l_lstm_para = GRU(100, return_sequences=True,implementation=2, dropout=0.2, recurrent_dropout=0.2)(norm_doc)\n",
    "    norm_doc_1 = BatchNormalization()(l_lstm_para)\n",
    "    l_att_para = AttLayer()(norm_doc_1)\n",
    "    weighted_sum_doc = WeightedSum()([norm_doc_1, l_att_para])\n",
    "    batch_norm_doc = BatchNormalization()(weighted_sum_doc)\n",
    "    #drop_out_doc = Dropout(0.2)(batch_norm_doc)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid')(batch_norm_doc) \n",
    "\n",
    "    model = Model(doc_input, preds)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(x_train,y_train, Best_Model_Filepath,model,x_val, y_val):\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    auc_eval = AUCEvaluation(validation_data=(x_val, y_val), interval=1)\n",
    "    precision_eval = PrecisionEvaluation(validation_data=(x_val, y_val), interval=1)\n",
    "    \n",
    "    earlyStopping = EarlyStopping(monitor='Avg_Prec',patience = 5, verbose =2, mode ='max')\n",
    "    checkpoint = ModelCheckpoint(Best_Model_Filepath, monitor='Avg_Prec', verbose=2, save_best_only=True, mode ='max')\n",
    "    print(\"training start...\")\n",
    "    training = model.fit(x_train,y_train,epochs=50,batch_size=BATCH_SIZE, \\\n",
    "                         callbacks=[auc_eval, precision_eval, earlyStopping, checkpoint],class_weight = class_weights,validation_data=[x_val,y_val],verbose=2)\n",
    "    #auc_eval,\n",
    "    \n",
    "    print('training end...')\n",
    "    \n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data,labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = load_data('/home/jujun/fraudprediction_10k/data/y_train_20200214')\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = load_data('/home/jujun/fraudprediction_10k/data/y_test_20200214')\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indecis = load_data('/home/jujun/fraudprediction_10k/data/indices_train_20200214')\n",
    "print(len(train_indecis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indecis = load_data('/home/jujun/fraudprediction_10k/data/indices_test_20200214')\n",
    "print(len(test_indecis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[train_indecis]\n",
    "X_test = data[test_indecis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelName = '/home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200215'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.Series(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test= pd.Series(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicmodel = basicModel(embedding_matrix,MAX_NB_WORDS,MAX_PARA_LENGTH)\n",
    "training = trainModel(X_train,y_train,ModelName, basicmodel, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicmodel.load_weights(ModelName)\n",
    "pred = basicmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_test = average_precision_score(y_test, pred)\n",
    "ap_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_test = roc_auc_score(y_test, pred)\n",
    "auc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
