{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import _pickle as pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import operator\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Dropout, Activation, Input, Flatten, Concatenate, GlobalAveragePooling1D, LSTM, GRU, Bidirectional, dot, multiply, Lambda, TimeDistributed, Masking\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2,l1,l1_l2\n",
    "from keras.callbacks import Callback,EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, precision_recall_curve, classification_report,accuracy_score, auc, roc_curve, roc_auc_score, average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    f = open(data_name,'rb')\n",
    "    data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_data('/home/jujun/fraudprediction_10k/data/data_20200212_2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_para = []\n",
    "# #labels = []\n",
    "# for d in data:\n",
    "#     text_para.append(d[0])\n",
    "#     #labels.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_doc(x):\n",
    "#     return ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_doc = []\n",
    "# for d in text_para:\n",
    "#     text_doc.append(generate_doc(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARA_LENGTH = 250\n",
    "MAX_PARAS = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "#lr = 0.0005\n",
    "opt = optimizers.Adam(lr= 0.0005)\n",
    "#l1_reg = l1(1e-5)\n",
    "#l12_reg = l1_l2(l1 = 1e-6,l2 = 1e-6)\n",
    "BATCH_SIZE = 24\n",
    "#metrics_auc = {}\n",
    "metrics_prcs = {}\n",
    "num_folder = 5\n",
    "#date = '20200208'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "# tokenizer.fit_on_texts(text_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index = tokenizer.word_index\n",
    "# print('Total %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index_50000 = { k:v for (k,v) in word_index.items() if v<=50000 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/jujun/fraudprediction_10k/HAN/input_data/word_index_50000_20200215','wb') as fp:\n",
    "#     pickle.dump(word_index_50000,fp)\n",
    "# fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jujun/fraudprediction_10k/data/word_index_50000_20200215','rb') as fp:\n",
    "    word_index_50000 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_index_50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(value):\n",
    "    return {k:v for k,v in word_index_50000.items() if v == value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.zeros((len(text_doc), MAX_PARAS, MAX_PARA_LENGTH), dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, doc in enumerate(text_para):\n",
    "#     #structure of a doc: [p1,p2,p3,p4]\n",
    "#     for j, para in enumerate(doc):\n",
    "        \n",
    "#         if j < MAX_PARAS:\n",
    "#             wordTokens = text_to_word_sequence(para)\n",
    "            \n",
    "#             k = 0\n",
    "#             for _, word in enumerate(wordTokens):\n",
    "#                 if k < MAX_PARA_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "#                     data[i, j, k] = tokenizer.word_index[word]\n",
    "#                     k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/home/jujun/fraudprediction_10k/data/handata_20200215.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('handata_20200215', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_w2v = [x.split(' ') for x in text_doc]\n",
    "# from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get my W2V model\n",
    "#Parameter: window=5,size=EMBEDDING_DIM, sg=1, workers=os.cpu_count(),min_count=5\n",
    "# w2v_model = Word2Vec(doc_w2v, window=5, workers=os.cpu_count(), min_count=5, size=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# embedding_matrix = np.random.random(((MAX_NB_WORDS+ 1) , EMBEDDING_DIM))\n",
    "# ignored=[]\n",
    "# cnt=0\n",
    "# for word, i in word_index.items():\n",
    "#     if word in w2v_model.wv.vocab and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "#         embedding_vector = w2v_model[word]\n",
    "#         cnt+=1\n",
    "#         if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "#     else: ignored.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120152"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49334"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('/home/jujun/fraudprediction_10k/HAN/input_data/embedding_matrix_20200215', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.save('/home/jujun/fraudprediction_10k/HAN/input_data/w2v_model_202002015.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.load('/home/jujun/fraudprediction_10k/data/embedding_matrix_20200215.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measure(pred_yp, y):\n",
    "    '''\n",
    "    Given lists of predicted y probability and x, y, return a dataframe of AR, AUC, Brier, Decile Table\n",
    "    '''\n",
    "    \n",
    "    tenc_dat = pd.DataFrame({'y_true':y,'probability':pred_yp.flatten()})\n",
    "    tenc_dat.sort_values('probability',axis = 0,ascending=False, inplace = True)\n",
    "    tenc_dat.index = range(0,len(tenc_dat))\n",
    "    y = tenc_dat['y_true']\n",
    "    point = float(len(tenc_dat))/10\n",
    "    point = int(round(point))\n",
    "    tenc = []\n",
    "    for i in range(0,10):\n",
    "        tenc.append(y[(i*point):((i+1)*point)])\n",
    "    tenc[9]=tenc[9].append(y[10*point:])\n",
    "    total = sum(y)\n",
    "    num_of_bkr = []\n",
    "    for j in range(0,10):\n",
    "        num_of_bkr.append(sum(tenc[j]))\n",
    "    tencile_bkr = np.array(num_of_bkr)\n",
    "    rate = tencile_bkr.astype(float)/total\n",
    "\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUCEvaluation(Callback):\n",
    "    \"\"\" Show AUC after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logs['auc'] = score\n",
    "            tencile=performance_measure(y_pred, self.y_val)\n",
    "            logs['tencile'] = tencile\n",
    "            print(\" epoch:{:d} AUC: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionEvaluation(Callback):\n",
    "    \"\"\" Show average Precision after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = average_precision_score(self.y_val, y_pred)\n",
    "            logs['Avg_Prec'] = score\n",
    "            #tencile=performance_measure(y_pred, self.y_val)\n",
    "            #logs['tencile'] = tencile\n",
    "            #print('prec_socre',score)\n",
    "            print(\" epoch:{:d} Avg_Prec: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUCEvaluation(Callback):\n",
    "    \"\"\" Show AUC after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logs['auc'] = score\n",
    "            tencile=performance_measure(y_pred, self.y_val)\n",
    "            logs['tencile'] = tencile\n",
    "            print(\" epoch:{:d} AUC: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionEvaluation(Callback):\n",
    "    \"\"\" Show average Precision after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = average_precision_score(self.y_val, y_pred)\n",
    "            logs['Avg_Prec'] = score\n",
    "            #tencile=performance_measure(y_pred, self.y_val)\n",
    "            #logs['tencile'] = tencile\n",
    "            #print('prec_socre',score)\n",
    "            print(\" epoch:{:d} Avg_Prec: {:.4f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    \n",
    "    def __init__(self, regularizer=None,context_dim=100, name=\"attention\",**kwargs):\n",
    "        self.regularizer = regularizer\n",
    "        self.context_dim=context_dim\n",
    "        self.supports_masking = True\n",
    "        self.name=name\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3        \n",
    "        self.W = self.add_weight(name='W', shape=(input_shape[-1], self.context_dim), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.b = self.add_weight(name='b', shape=(self.context_dim,), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.u = self.add_weight(name='u', shape=(self.context_dim,), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)        \n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        eij = K.squeeze(K.dot(eij, K.expand_dims(self.u, axis=1)), axis=-1)\n",
    "        #eij = K.dot(K.tanh(K.dot(x, self.W) + self.b), self.u)\n",
    "        ai = K.exp(eij)\n",
    "        #print(\"ai size\", K.int_shape(ai))\n",
    "        #print(\"mask size\", K.int_shape(mask))\n",
    "        \n",
    "        if mask is not None:\n",
    "            ai*=K.cast(mask, K.floatx())\n",
    "            \n",
    "        ai /=K.cast(K.sum(ai, axis=1, keepdims=True)+K.epsilon(), K.floatx())\n",
    "        #weights=K.expand_dims(weights, axis=2) # (None, 1000,1)\n",
    "        #alphas = ai / K.sum(ai, axis=1).dimshuffle(0, 'x')\n",
    "        \n",
    "        #weighted_input = x * K.expand_dims(ai)\n",
    "        #return K.sum(weighted_input, axis=1)\n",
    "        return ai\n",
    "        \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(AttLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(Layer):\n",
    "    def __init__(self, name=\"weighted_sum\",  **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.name=name\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #assert isinstance(input_shape, list)\n",
    "        #print(\"input shape\", input_shape)\n",
    "        super(WeightedSum, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        \n",
    "        x = input_tensor[0]\n",
    "        #print(\"input\",K.int_shape(x))\n",
    "        a = input_tensor[1]\n",
    "        print(\"weights\", K.int_shape(a))\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = K.sum(x * a, axis=1)\n",
    "        #print(\"weighted sum\",K.int_shape(weighted_input))\n",
    "        \n",
    "        return weighted_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        a, b = input_shape\n",
    "        \n",
    "        return (a[0], a[-1])\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicModel(embedding_matrix,MAX_NB_WORDS,MAX_PARA_LENGTH,):\n",
    "    embedding_layer = Embedding((MAX_NB_WORDS+ 1),\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            mask_zero=True,\n",
    "                            input_length=MAX_PARA_LENGTH,\n",
    "                            trainable=True)    \n",
    "    \n",
    "    \n",
    "    para_input = Input(shape=(MAX_PARA_LENGTH, ), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(para_input)\n",
    "    norm_sequence = BatchNormalization()(embedded_sequences)\n",
    "    drop_out = Dropout(0.2)(norm_sequence)     \n",
    "    l_att = AttLayer()(drop_out)\n",
    "    weighted_sum = WeightedSum()([norm_sequence,l_att])\n",
    "    paraEncoder =Model(para_input,weighted_sum)\n",
    "    paraEncoder.summary()\n",
    "    \n",
    "    doc_input = Input(shape=(MAX_PARAS, MAX_PARA_LENGTH), dtype='int32')\n",
    "    doc_encoder = TimeDistributed(paraEncoder)(doc_input)\n",
    "    mask_doc = Masking(mask_value=0.0)(doc_encoder)\n",
    "    norm_doc = BatchNormalization()(mask_doc)\n",
    "    #update the dropout layer for the GRU\n",
    "    l_lstm_para = GRU(100, return_sequences=True,implementation=2, dropout=0.2)(norm_doc)\n",
    "    norm_doc_1 = BatchNormalization()(l_lstm_para)\n",
    "    l_att_para = AttLayer()(norm_doc_1)\n",
    "    weighted_sum_doc = WeightedSum()([norm_doc_1, l_att_para])\n",
    "    batch_norm_doc = BatchNormalization()(weighted_sum_doc)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid')(batch_norm_doc) \n",
    "\n",
    "    model = Model(doc_input, preds)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(x_train,y_train, Best_Model_Filepath,model,x_val, y_val):\n",
    "    \n",
    "    \n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    val_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                             np.unique(y_val),\n",
    "                                             y_val)\n",
    "#     val_class_weights_dict = {}\n",
    "#     class_weights_dict[0] = class_weights[0]\n",
    "#     class_weights_dict[1] = class_weights[1]\n",
    "    val_sample_weights = []\n",
    "    for y in y_val:\n",
    "        if y == 1:\n",
    "            val_sample_weights.append(val_class_weights[1])\n",
    "        else: val_sample_weights.append(val_class_weights[0])\n",
    "    val_sample_weights = np.asarray(val_sample_weights)\n",
    "    \n",
    "    print(\"shape of weights: \",val_sample_weights.shape, \"shape y_val: \", y_val.shape)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer= opt,\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    auc_eval = AUCEvaluation(validation_data=(x_val, y_val), interval=1)\n",
    "    precision_eval = PrecisionEvaluation(validation_data=(x_val, y_val), interval=1)\n",
    "    \n",
    "    earlyStopping = EarlyStopping(monitor='Avg_Prec',patience = 5, verbose =2, mode ='max')\n",
    "    checkpoint = ModelCheckpoint(Best_Model_Filepath, monitor='Avg_Prec', verbose=2, save_best_only=True, mode ='max')\n",
    "    print(\"training start...\")\n",
    "    training = model.fit(x_train,y_train,epochs=50,batch_size=BATCH_SIZE, \\\n",
    "                         callbacks=[auc_eval, precision_eval, earlyStopping, checkpoint],class_weight = class_weights, \\\n",
    "                         validation_data=[x_val,y_val,val_sample_weights],verbose=2)\n",
    "    #auc_eval,\n",
    "    \n",
    "    print('training end...')\n",
    "    \n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(data,labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45030\n"
     ]
    }
   ],
   "source": [
    "train_labels = load_data('/home/jujun/fraudprediction_10k/data/y_train_20200214')\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11258\n"
     ]
    }
   ],
   "source": [
    "test_labels = load_data('/home/jujun/fraudprediction_10k/data/y_test_20200214')\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45030\n"
     ]
    }
   ],
   "source": [
    "train_indecis = load_data('/home/jujun/fraudprediction_10k/data/indices_train_20200214')\n",
    "print(len(train_indecis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11258\n"
     ]
    }
   ],
   "source": [
    "test_indecis = load_data('/home/jujun/fraudprediction_10k/data/indices_test_20200214')\n",
    "print(len(test_indecis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[train_indecis]\n",
    "X_test = data[test_indecis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56288, 200, 250)\n",
      "(45030, 200, 250)\n",
      "(11258, 200, 250)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelName = '/home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200215'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.Series(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45030,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test= pd.Series(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isfinite(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isfinite(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isfinite(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isfinite(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainset, X_val, y_trainset, y_val = train_test_split( X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36024,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9006,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights (None, 250)\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 250, 100)     5000100     input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 250, 100)     400         embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 250, 100)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_21 (AttLayer)         (None, 250)          10200       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "weighted_sum_21 (WeightedSum)   (None, 100)          0           batch_normalization_41[0][0]     \n",
      "                                                                 att_layer_21[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 5,010,700\n",
      "Trainable params: 5,010,500\n",
      "Non-trainable params: 200\n",
      "__________________________________________________________________________________________________\n",
      "weights (None, 250)\n",
      "weights (None, 200)\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           (None, 200, 250)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 200, 100)     5010700     input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_11 (Masking)            (None, 200, 100)     0           time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 200, 100)     400         masking_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "gru_11 (GRU)                    (None, 200, 100)     60300       batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 200, 100)     400         gru_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_22 (AttLayer)         (None, 200)          10200       batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "weighted_sum_22 (WeightedSum)   (None, 100)          0           batch_normalization_43[0][0]     \n",
      "                                                                 att_layer_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 100)          400         weighted_sum_22[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            101         batch_normalization_44[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 5,082,501\n",
      "Trainable params: 5,081,701\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n",
      "shape of weights:  (9006,) shape y_val:  (9006,)\n",
      "training start...\n",
      "Train on 36024 samples, validate on 9006 samples\n",
      "Epoch 1/50\n",
      " - 958s - loss: 0.0699 - acc: 0.9858 - val_loss: 2.5070 - val_acc: 0.9898\n",
      " epoch:0 AUC: 0.6297\n",
      " epoch:0 Avg_Prec: 0.0511\n",
      "\n",
      "Epoch 00001: Avg_Prec improved from -inf to 0.05106, saving model to /home/jujun/fraudprediction_10k/HAN/hanmodel_dropout_20200215\n",
      "Epoch 2/50\n"
     ]
    }
   ],
   "source": [
    "basicmodel = basicModel(embedding_matrix,MAX_NB_WORDS,MAX_PARA_LENGTH)\n",
    "training = trainModel(X_trainset, y_trainset, ModelName, basicmodel, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicmodel.load_weights(ModelName)\n",
    "pred = basicmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_test = average_precision_score(y_test, pred)\n",
    "ap_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_test = roc_auc_score(y_test, pred)\n",
    "auc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
